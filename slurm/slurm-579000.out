Message from TWCC HPC admin
-----------------------
loading miniconda3 with conda 4.8.4/python 3.7
docs : https://hackmd.io/@kmo/twcc_hpc_conda
-----------------------

# conda environments:
#
                         /home/kodmas2023/miniconda3/envs/ape
                         /home/kodmas2023/miniconda3/envs/ape2
base                  *  /opt/ohpc/twcc/conda/4.8.3/miniconda3

/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
[INFO] : Generating prompts from llama2 ...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.52s/it]
Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.
make_response
Traceback (most recent call last):
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 748, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 720, in as_tensor
    return torch.tensor(value)
ValueError: expected sequence of length 144 at dim 1 (got 135)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/work/kodmas2023/automatic_prompt_engineer/slurm/../experiments/run_instruction_induction.py", line 127, in <module>
    fire.Fire(run)
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/work/kodmas2023/automatic_prompt_engineer/slurm/../experiments/run_instruction_induction.py", line 63, in run
    res, demo_fn = ape.find_prompts(eval_template=eval_template,
  File "/work/kodmas2023/automatic_prompt_engineer/automatic_prompt_engineer/ape.py", line 170, in find_prompts
    prompts = generate.generate_llama2_prompts(
  File "/work/kodmas2023/automatic_prompt_engineer/automatic_prompt_engineer/generate.py", line 68, in generate_llama2_prompts
    prompts = llama2_model.make_response(queries)
  File "/work/kodmas2023/automatic_prompt_engineer/automatic_prompt_engineer/llm.py", line 102, in make_response
    inputs = self.tokenizer(
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2798, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2884, in _call_one
    return self.batch_encode_plus(
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3075, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py", line 552, in _batch_encode_plus
    return BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type=return_tensors)
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 223, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/kodmas2023/miniconda3/envs/ape/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 764, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
